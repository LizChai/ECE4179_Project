{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ea2915-6283-4a60-8b7f-71ebb59465c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ieliz\\appdata\\roaming\\python\\python38\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Collecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\ieliz\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.0.17 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ccaf5-3cfc-4b04-9a88-1ef14abfcc75",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30db8af7-09a9-4ce0-8c98-8a31f275ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel , GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4382c-4b8f-40ee-94a7-b9edb6cbcd5f",
   "metadata": {},
   "source": [
    "# Download tokens and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c32b0b6-e4a2-4c64-a190-ba49f4fe6c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3607f49797dd43b1a1d0e95b50084012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc31d567985f457aa042d5bb25698cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bcda94b9c44953b606e8b4b2c83b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9757bc7b3b3447198ab51d90c0beffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fec1d75ef34e3eb4f6e4e3cd396dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large') \n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-large', pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f68283-7b5c-48a3-bbbf-c1e0fcacb183",
   "metadata": {},
   "source": [
    "# Test encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152110f0-00d0-4742-8cc9-5a929f96412a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 464, 6193,  318, 3621]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode text and return torch tensors 'pt' (PyTorch tensors)\n",
    "# Converts words into numbers (indices)\n",
    "topic = \"The weather is nice\"\n",
    "input_ids = tokenizer.encode(topic, return_tensors = 'pt')\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72938db0-bfb6-4c05-a58b-d4a6930386ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The weather is nice'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode text from indices\n",
    "tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e400ddcd-7df1-4411-b2c8-182589e99fc0",
   "metadata": {},
   "source": [
    "# Generate text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "478f3819-7001-473a-99ed-a74c77b39740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb79f89f-6ead-4921-9313-0d56b2016cd0",
   "metadata": {},
   "source": [
    "## Beam Search text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3847e8b0-5aa3-4d26-9efb-fe5fb1c29ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using generate function from GPT2LMHeadModel via beam search\n",
    "# https://huggingface.co/blog/how-to-generate\n",
    "# Args: max_length: maximum number of words in generated text\n",
    "#       num_beams: beam search reduces risk of missing hidden high probability word sequences by keeping the most\n",
    "#                  likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has \n",
    "#                  the overall highest probability\n",
    "#       no_repeat_ngram_size: while result is arguably more fluent, output still includes repetition of same word seqs\n",
    "#                  introduce n-grams (word seqs of n words) penalties\n",
    "output_beam = model.generate(input_ids, max_length = 500, num_beams = 5, \n",
    "                        no_repeat_ngram_size  = 2, early_stopping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d320bc5-fe40-4285-89f0-04a9682cd50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is nice today, but it's going to be a long day.\"\n",
      "\n",
      "\"Yeah, I know. I'm just glad we're not in the middle of a snowstorm. It would have been a lot worse if we were in a blizzard or something like that. We could have gotten stuck in there for a while, and I don't think we'd be able to get out of there. But I guess we'll just have to wait and see what the weather's like tomorrow, then we can decide whether or not we want to go back out there.\" She smiled at me. \"I'm glad you're okay with it, though. You're the first person I've told about this, after all.\" I smiled back at her. She was a good friend, even if she wasn't always the best person to talk to about things like this. Maybe that was why I liked her so much, because she was always willing to listen to me and try to help me out when I was in trouble. That was something I really appreciated about her, in addition to the fact that she always seemed to know what was going on in my head and what I needed to do to make things better. Even though she didn't know everything, she knew enough to give me advice and make me feel better about myself. And that made me really happy, especially when she would tell me things that I would never have thought of on my own, like how I shouldn't be afraid of the dark, or that it was okay to cry when you were sad. Or that if you had a crush on someone, you should just let it go and move on with your life, instead of trying to force it on them. Those were the kinds of things she told me all the time, which made it really easy for me to believe in her and her advice. When I told her about how much I wanted to see her again, it made her smile even wider, as if to say, \"Oh, that's so sweet of you to think of me that way. Of course I'd want you back, of course you would want me back.\" And then she hugged me tightly and kissed me on the cheek, saying \"Goodbye, sweetie. See you tomorrow.\" Then she turned around and walked away, leaving me alone with my thoughts. After a few minutes of thinking about what she had just said, my mind started to wander back to what had happened earlier.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output_beam[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab322c6-418a-4518-bc51-2bcca8004b11",
   "metadata": {},
   "source": [
    "## Top-K sampling text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c2b6874-8771-4c61-9292-968b8de91060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using generate function from GPT2LMHeadModel via Top-K sampling\n",
    "# K most likely next words are filtered and probability mass is redistributed among only those K next words\n",
    "# Method adopted by GPT2\n",
    "output_topk = model.generate(input_ids, do_sample=True, max_length = 500, top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc7eaff7-5cfb-4ef1-b039-eb5a9480a25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is nice, not too hot, not too cold. Lots to do, there are always new friends to welcome you and lots to hear about and see. We are a wonderful group. I was so surprised by this that I said \"Oh wow.\" I knew I would see a bunch of old friends, but at that time I thought it would be just a bunch of old people. I was so shocked that I said it out loud at that moment.\n",
      "\n",
      "The main bar is always crowded, and I often do a lot of drinking after walking down the narrow and winding streets. I do know of a couple, maybe even three that have taken a long break from the town over the years, for the time being. I was very surprised that they stopped, as I had no idea that they had not actually moved, that they really just decided to not come back and get things on track for the town. They told me that when I came from the hotel to the bar, in the evening the people who owned the motel didn't invite me. I knew that all the other people in town who were taking their vacations at the resort felt the same. It was an obvious reason. I thought that I should go see where they did stay, and see if their house has been torn down, as I did know that the people who own the hotel have taken a huge interest in the community, but decided against it to be honest. I tried visiting the site where the old school's buildings used to stand, but I decided to get a picture from a car on the side of the road. The owner of the house owns a place with a restaurant and a lot of old memorabilia of the residents of the town. The old school is still there, and will probably be the place for some time, so I had the pictures from the restaurant taken. There were some old pictures I had missed in my long road to town. These pictures I was told would still have a sentimental value for people who grew up in the 1960's. So one evening after seeing those pictures at the house where the old school used to be, I walked back to the motel a little more than half a mile to my car, where I met a few of my old friends from the town.\n",
      "\n",
      "After we had lunch, we all headed up into the mountains. We were allowed to use the bathrooms and the elevations seemed to be easy to figure out, which was great, and I didn't\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output_topk[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aed3a0-904e-4287-bd0e-4170e0aace34",
   "metadata": {},
   "source": [
    "## Top-p sampling text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bef3dca-93d7-4a98-9042-71f233f19118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using generate function from GPT2LMHeadModel via Top-p sampling\n",
    "# Aka nucleus-sampling; top-p sampling chooses the smallest possible set of words whose cumulative property\n",
    "# exceeds the probability p; probability mass is redistributed among this set of words\n",
    "# This means, the size od the set of words can be dynamically increased and decreased according to \n",
    "# next word's probability distribution\n",
    "output_topp = model.generate(input_ids, do_sample=True, max_length=500, top_p=0.92, top_k=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5732244a-a6ea-40cb-922b-90b8c2af8902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is nice. Thank God it's sunny.'\n",
      "\n",
      "His daughter, Amanda, said: 'When we called he said he had gone out on a date and had taken the wrong car, but he'd been driving the car for the last few hours.'\n",
      "\n",
      "Mr Scheffler, from St Albans, Gloucestershire, posted his frustration at Facebook at 8.40pm. He said: 'When our 11-month-old son Kat popped into the shop and asked to play with a toy hammer-head, we went to meet him at the gates.\n",
      "\n",
      "'He was fitting it to a piece of steel and showed me how he kept it together - fixing the hammer head and using it as a gear up and down.'\n",
      "\n",
      "He said he tried to help his son 'but I couldn't, and on my return to the car a couple of minutes later I knew what had happened and only a few minutes later his little body was no longer in there.'\n",
      "\n",
      "His wife Amanda said: 'Kat is an idiot. Of course I can understand why he was annoyed with us and would have done something crazy, but to do that in front of his parents with his parents not getting it was horrible.'\n",
      "\n",
      "Kat's mum Aune, who lives in Lahore, Pakistan, confirmed the car arrived at the shop 'by mistake' - but his girlfriend was waiting at the factory in Cheltenham to collect the car.\n",
      "\n",
      "She said: 'It had been coming back here to the UK for two or three days to start work - and we had no idea it would be in this back garden. We assumed it was brought back by one of the girls in the factory, until Kat arrived.\n",
      "\n",
      "The company director does not want to be named but insisted he told his workers to put their luggage at the front gate so as not to stir up any trouble during their shift. He added that his staff treated him with 'honour and dignity'\n",
      "\n",
      "Angry dad Glyn, 44, said his son, 11, was being 'dropped off by a friend' at the factory and was happy\n",
      "\n",
      "'We're their guests and we should have known better. We're baffled, devastated.'\n",
      "\n",
      "Keith Clark, 24, a farmer who saw the incident, said: 'The family clearly got too much for our young lad and he didn't make it back to me.\n",
      "\n",
      "'He was wrapped in a towel\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output_topp[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ce38f-c758-48c3-9b8f-b4b6a30746e9",
   "metadata": {},
   "source": [
    "## Using both Top-k and Top-p in text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2d521c5-f4be-47c4-b3da-b62199985677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both work well in practice, so lets use both toegher to avoid very low ranked words while allowing for some dynamic selection\n",
    "output_topkp = model.generate(input_ids, do_sample=True, max_length=500, top_k=50, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "512aa5c0-955b-4199-9a8b-46e8a8a8e0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is nice today. So, are you ready to come and visit our city?\"\n",
      "\n",
      "Jian Chen nodded. As if to say, \"You are welcome, brother.\"\n",
      "\n",
      "A simple matter like that, there was no need to explain it. A few moments later, a group of youths emerged from the gates and walked into the city gates. The people inside the city immediately looked towards the direction of the city gates and saw that they were a group of youths and a woman. The young man holding onto her looked very dignified and noble.\n",
      "\n",
      "She was only 21 years old, but her beauty was like the sea and clouds, so clear that people could see across the continent without any effort.\n",
      "\n",
      "This woman, on the other hand, was an extremely cold and indifferent woman. She was extremely handsome, but only an average height of 182 centimeters. Her blue eyes were also extremely cold and icy as they looked in the direction of the city gates. She had a cold expression on her face, but the most shocking aspect was the expression on her hands. Her hands were always curled up and her fingernails were always painted black. This extremely cold look was also the reason why people who saw this scene would always call this woman \"the cold hand\".\n",
      "\n",
      "However, there were people who were extremely familiar with her face and did not know who she was, so they were actually also calling her the cold hand. However, this time there was no coldness in her expression. In fact, she looked even more respectful to the city's lord, Lin Xiu and his family. She looked at him with a gaze that seemed like the coldest and most respectful eyes she could ever look at.\n",
      "\n",
      "This was also the reason why this woman had attracted the attention of a large number of people, including those who had come to visit this city.\n",
      "\n",
      "These were not ordinary people either. If she really dared to show off a smile that looked like it was an ice-cold expression of contempt, her status would be directly smashed.\n",
      "\n",
      "The people in the city immediately saw this woman in a great hurry and hurriedly waved for her to stop. She took a step forward and turned away from them. The people who were still unable to see clearly also followed behind her.\n",
      "\n",
      "\"I'm from the Yu family,\" the young man holding onto the woman said coldly, his hands behind his back. His face was still very ugly,\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output_topkp[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc0045a-5cc6-47c3-a963-3564ef924f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
